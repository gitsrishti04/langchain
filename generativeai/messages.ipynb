{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e720b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishti/Desktop/langchain/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model =init_chat_model(\"groq:qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5408e582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user asked me to tell them something about AI. First, I need to figure out what their main need is. They might be new to AI, or maybe they want a brief overview. I should start with a basic definition and then expand into different areas.\\n\\nI should cover the main types of AI, like narrow AI versus general AI, and maybe touch on superintelligent AI. It\\'s important to explain the key technologies involved, such as machine learning, deep learning, and neural networks. Including examples of real-world applications could help make it more relatable, like image recognition, natural language processing, and autonomous vehicles.\\n\\nI should also mention the history a bit, like when AI research started and some milestones. Ethical considerations are crucial too‚Äîdata privacy, bias, and job displacement. It\\'s good to present both the benefits and the challenges of AI development.\\n\\nI need to structure this in a way that\\'s easy to follow, maybe starting with the definition, then types, technologies, applications, history, ethics, and the future. Keep each section concise to avoid overwhelming the user. Also, use simple language and avoid jargon as much as possible. Let me check if there\\'s anything else they might want. Maybe mention current trends or companies leading in AI? Or perhaps touch on how AI is changing industries. Yeah, that could add value. Make sure to wrap it up by summarizing the impact and future potential of AI.\\n</think>\\n\\nArtificial Intelligence (AI) is a branch of computer science focused on creating systems capable of performing tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, language understanding, and decision-making. Here\\'s an overview of key aspects of AI:\\n\\n---\\n\\n### **1. Core Concepts**\\n- **Narrow AI (Weak AI)**: Designed for specific tasks (e.g., voice assistants like Siri, recommendation algorithms, or chess-playing programs). Most current AI systems fall into this category.\\n- **General AI (Strong AI)**: A hypothetical AI that can perform *any* intellectual task a human can do. It does not yet exist.\\n- **Superintelligent AI**: An AI surpassing human capabilities in all domains, a concept often explored in science fiction and future scenarios.\\n\\n---\\n\\n### **2. Key Technologies**\\n- **Machine Learning (ML)**: Algorithms that learn patterns from data (e.g., predicting house prices from historical sales data).\\n- **Deep Learning**: A subset of ML using neural networks with multiple layers to model complex patterns (e.g., image recognition, speech synthesis).\\n- **Natural Language Processing (NLP)**: Enables machines to understand and generate human language (e.g., chatbots like me!).\\n- **Computer Vision**: Allows machines to interpret and analyze visual data (e.g., facial recognition, autonomous vehicles).\\n- **Robotics**: Combines AI with physical systems to perform tasks in the real world (e.g., warehouse automation).\\n\\n---\\n\\n### **3. Applications**\\nAI is transforming industries through:\\n- **Healthcare**: Diagnosing diseases, drug discovery, and personalized treatment.\\n- **Finance**: Fraud detection, algorithmic trading, and risk management.\\n- **Transportation**: Autonomous vehicles and traffic optimization.\\n- **Retail**: Personalized shopping, inventory management, and demand forecasting.\\n- **Entertainment**: Content recommendation (Netflix), game development, and AI-generated art.\\n- **Customer Service**: Chatbots and virtual assistants for 24/7 support.\\n\\n---\\n\\n### **4. History & Milestones**\\n- **1950s‚Äì1970s**: Birth of AI with the Dartmouth Conference (1956) and early programs like the Logic Theorist.\\n- **1980s‚Äì1990s**: Expert systems and rule-based AI, alongside setbacks due to computational limits (\"AI Winter\").\\n- **2000s‚Äì2010s**: Renewed growth via big data, cloud computing, and breakthroughs like deep learning (e.g., AlphaGo defeating a Go champion in 2016).\\n- **2020s**: Explosion of generative AI models (e.g., GPT, DALL¬∑E) and AI integration into daily life.\\n\\n---\\n\\n### **5. Ethical & Societal Challenges**\\n- **Bias & Fairness**: AI can inherit biases from training data, leading to unfair outcomes.\\n- **Privacy**: Risks of data misuse in surveillance or personalization.\\n- **Job Displacement**: Automation may disrupt labor markets, though it also creates new roles.\\n- **Accountability**: Determining responsibility for AI errors (e.g., autonomous vehicle accidents).\\n- **Security**: Risks of AI misuse (e.g., deepfakes, cyberattacks).\\n\\n---\\n\\n### **6. Future Directions**\\n- **AI Alignment**: Ensuring AI systems act in humanity\\'s best interest.\\n- **Explainability**: Making AI decisions transparent and interpretable.\\n- **Collaborative AI**: Tools that augment human capabilities (e.g., AI in healthcare diagnostics).\\n- **Regulation**: Global efforts to establish ethical guidelines and legal frameworks.\\n\\n---\\n\\n### **7. Leading Companies & Research**\\n- **Tech Giants**: Google (DeepMind), Microsoft, Meta, and OpenAI.\\n- **Academic Institutions**: MIT, Stanford, and Carnegie Mellon.\\n- **Startups**: Companies like Anthropic (Claude), Stability AI, and others pushing AI innovation.\\n\\n---\\n\\n### **Why It Matters**\\nAI has the potential to solve complex global challenges (e.g., climate change, healthcare access) but also raises critical questions about ethics, governance, and equity. Its development requires balancing innovation with responsibility.\\n\\n---\\n\\nWould you like to dive deeper into a specific area, like AI ethics, technical details, or real-world examples? üòä', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1180, 'prompt_tokens': 14, 'total_tokens': 1194, 'completion_time': 2.429949053, 'completion_tokens_details': None, 'prompt_time': 0.000574102, 'prompt_tokens_details': None, 'queue_time': 0.051705345, 'total_time': 2.430523155}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2560-9d3a-7720-ab9d-8432a8abb667-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 14, 'output_tokens': 1180, 'total_tokens': 1194})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Please tell me something about AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f1e3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, I need to explain what LangChain is. Let me start by recalling what I know. LangChain is a framework related to working with language models, maybe for building applications that use large language models like GPT. I think it\\'s about connecting LLMs with other components to create more complex systems.\\n\\nFirst, I should mention that LangChain helps with integrating LLMs into applications. It probably provides tools for things like prompts, memory, and data processing. I remember something about chains, agents, and memory systems. Chains might be sequences of operations that use the LLM, like taking an input, processing it through a chain of steps, and producing an output.\\n\\nAgents are another part. They might allow the LLM to make decisions about what actions to take, maybe interacting with external tools or APIs. For example, an agent could decide to look up information on the web or retrieve data from a database before providing an answer.\\n\\nMemory is another aspect. LangChain might have mechanisms for storing and retrieving past interactions, so the system can remember context over time. This could help in maintaining a conversation state or providing more personalized responses.\\n\\nData processing is also important. LangChain probably includes tools for handling data, like document loaders for reading from various sources (PDFs, text files, etc.), and embeddings for converting text into numerical representations that can be processed by models.\\n\\nI should also mention that LangChain supports integration with different LLMs, not just one. That means developers can switch between models without rewriting their entire application. There might be modules like Prompt Templates for creating structured prompts, which help in getting more accurate responses from the LLM.\\n\\nUse cases could include chatbots, data analysis tools, automated customer service, or any application where a language model needs to be combined with other data sources or tools. Examples might be a chatbot that can search the web for answers or a document summarizer that extracts key points from long texts.\\n\\nI need to make sure I cover the main components: Chains, Agents, Memory, Data Processing, and Integrations. Also, maybe mention the modularity and flexibility of the framework, allowing developers to build complex workflows by combining these components.\\n\\nWait, I should verify if I\\'m mixing up any concepts. For example, Chains in LangChain are sequences of steps that can include LLM calls, data transformations, etc. They can be linear or have branches. Agents, on the other hand, are more about autonomy, where the system can decide which actions to take based on the input. Memory is about storing and retrieving information across interactions. Data Processing involves handling external data, maybe using tools like LangChain\\'s document loaders and text splitters.\\n\\nI should also mention the ecosystem: tools for working with databases, APIs, etc., to allow the LLM to interact with external systems. For example, using a tool to query a SQL database based on user input.\\n\\nAnother point is the Prompt Templates, which help in creating consistent and effective prompts for the LLM. This is important for getting reliable outputs, especially when the LLM needs to follow specific formats or guidelines.\\n\\nTesting and evaluation might be part of LangChain as well, providing ways to assess the performance of the chains or agents.\\n\\nI need to structure this in a clear, logical way. Start with a definition, then list the main components, explain each briefly, and mention use cases. Make sure to highlight the benefits like flexibility, integration with multiple models, and the ability to add memory and tools.\\n\\nWait, maybe start with a simple definition, then go into key components, then examples of applications. Avoid technical jargon where possible, but since it\\'s about a framework, some terms are necessary. Make sure to clarify terms like chains, agents, etc., in simple language.\\n\\nI should also check if there are any other important aspects I\\'m missing. Maybe the modular architecture allows for easy experimentation and iteration. Also, LangChain might have a community or ecosystem with plugins and extensions.\\n\\nI think that\\'s a solid outline. Now, structure the answer accordingly, ensuring each part is explained clearly and concisely.\\n</think>\\n\\n**LangChain** is a framework designed to simplify the development of applications that leverage **Large Language Models (LLMs)** like GPT, Llama, or others. It provides tools, components, and integrations to build complex workflows by connecting LLMs with external tools, data sources, and systems. Its flexibility allows developers to create everything from simple chatbots to advanced AI-driven applications.\\n\\n---\\n\\n### **Key Components of LangChain**\\n1. **Chains**  \\n   - **Definition**: Sequences of steps (LLM calls, data transformations, or other operations) that process inputs and produce outputs.  \\n   - **Example**: A chain might take a user query, generate an answer using an LLM, then format the response.  \\n   - **Types**:  \\n     - **Simple Chains**: A single LLM call with input/output.  \\n     - **Sequential Chains**: Multiple steps executed in order.  \\n     - **Conditional Chains**: Branch logic based on input.\\n\\n2. **Agents**  \\n   - **Definition**: Autonomous systems that decide which actions to take (e.g., querying databases, web searches) to achieve a goal.  \\n   - **Example**: An agent could search the web for recent news about a topic before summarizing it for the user.  \\n   - **Key Feature**: Uses the LLM to \"think\" about the best action step-by-step.\\n\\n3. **Memory**  \\n   - **Definition**: Mechanisms to store and retrieve context (e.g., conversation history) across interactions.  \\n   - **Types**:  \\n     - **Short-Term Memory**: Tracks recent interactions (e.g., chat history).  \\n     - **Long-Term Memory**: Stores data for persistent use (e.g., user preferences).\\n\\n4. **Data Processing**  \\n   - **Tools**:  \\n     - **Document Loaders**: Extract text from files (PDFs, websites, etc.).  \\n     - **Text Splitters**: Break large texts into manageable chunks.  \\n     - **Embeddings**: Convert text into numerical vectors for tasks like similarity searches.  \\n   - **Use Case**: Build a Q&A system that retrieves answers from a document database.\\n\\n5. **Integrations**  \\n   - **LLM Support**: Works with multiple models (e.g., OpenAI, Hugging Face, Anthropic).  \\n   - **External Tools**: Connects to APIs, databases, or tools like Google Search, SQL, and more.  \\n   - **Prompt Templates**: Customizable prompts to guide the LLM‚Äôs output (e.g., forcing a specific format).\\n\\n---\\n\\n### **Why Use LangChain?**\\n- **Modularity**: Build applications by combining reusable components (chains, agents, memory).  \\n- **Flexibility**: Switch between LLMs or tools without rewriting code.  \\n- **Scalability**: Handle complex workflows (e.g., multi-step reasoning, data retrieval).  \\n- **Ecosystem**: Leverage existing tools (e.g., Google Search, SQL) and community plugins.\\n\\n---\\n\\n### **Example Use Cases**\\n1. **Chatbots with Memory**:  \\n   A customer service chatbot that remembers past interactions with a user.  \\n2. **RAG Systems (Retrieval-Augmented Generation)**:  \\n   Answer questions by first retrieving relevant documents from a database.  \\n3. **Automated Research Assistants**:  \\n   Agents that search the web, extract key points, and summarize findings.  \\n4. **Data Analysis Tools**:  \\n   Generate insights from datasets using LLMs and SQL database integrations.\\n\\n---\\n\\n### **Getting Started**\\nLangChain is open-source and supports Python. Key steps include:  \\n1. Install the library: `pip install langchain`.  \\n2. Define prompts, chains, or agents.  \\n3. Integrate with your chosen LLM and tools.  \\n\\nFor example, a simple chain might look like this:  \\n```python\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.llms import OpenAI\\n\\nprompt = PromptTemplate(input_variables=[\"topic\"], template=\"Explain {topic} in simple terms.\")\\nllm = OpenAI(model_name=\"gpt-3.5-turbo\")\\nchain = prompt | llm\\nresult = chain.invoke({\"topic\": \"quantum computing\"})\\n```\\n\\n---\\n\\n### **Conclusion**  \\nLangChain empowers developers to create intelligent, scalable applications by combining LLMs with tools, memory, and custom logic. Its modular design and strong community support make it a go-to framework for AI application development. Whether you\\'re building a chatbot, a data analysis tool, or an autonomous agent, LangChain provides the building blocks to bring your ideas to life.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1783, 'prompt_tokens': 12, 'total_tokens': 1795, 'completion_time': 5.245044864, 'completion_tokens_details': None, 'prompt_time': 0.000322705, 'prompt_tokens_details': None, 'queue_time': 0.050526154, 'total_time': 5.245367569}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2560-a751-7241-aa49-0da365330006-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 12, 'output_tokens': 1783, 'total_tokens': 1795})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what is langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c43623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to explain what RAG applications are. Let me start by recalling what I know. RAG stands for Retrieval-Augmented Generation. I remember it's a technique used in AI models, maybe in natural language processing. But how exactly does it work?\n",
      "\n",
      "From what I've read before, traditional language models generate text based solely on their training data. But with RAG, they retrieve information from external sources. So when a user asks a question, the model doesn't just answer from its own knowledge but looks up relevant information first. Then it uses that to generate a more accurate response. That makes sense. It's like combining a search engine with a language model.\n",
      "\n",
      "Wait, how does the retrieval part work? Maybe it uses a search engine or a database to find the most relevant documents. Then the generation part takes those documents and synthesizes the information into an answer. So the model is augmented with retrieved information to enhance its responses. That could help with tasks like answering questions where the answer isn't in the model's training data but is available in some external database.\n",
      "\n",
      "What are the components of a RAG system? There's the retrieval component, which could be based on things like BM25 or neural embeddings. Then there's the generation component, usually a large language model. The process would involve taking the user's query, retrieving relevant documents, then feeding both the query and documents into the generator to produce an answer.\n",
      "\n",
      "Applications of RAG might include customer support chatbots that can access a company's knowledge base, or virtual assistants that provide up-to-date information. It could also be used in research to help find relevant papers or in healthcare to retrieve medical records. The key idea is that the model isn't just relying on its prior knowledge but can also use current or specific data sources.\n",
      "\n",
      "But there are challenges too. The retrieval system needs to be efficient and accurate. If the documents retrieved are not relevant, the generated answer might be wrong. Also, integrating the retrieval and generation steps smoothly is important. There might be issues with latency if the retrieval takes too long. Plus, the model has to handle cases where multiple documents provide conflicting information.\n",
      "\n",
      "How is RAG different from just using a search engine and then a language model? Well, in a basic setup, you might search first and then summarize, but RAG does this in a more integrated way. The retrieval and generation steps are part of a single system, possibly with training to optimize both components together. That could lead to better performance than using them separately.\n",
      "\n",
      "I should also mention the technical aspects. For example, in RAG, the retrieval can be done using dense vector representations (like with transformers) or traditional keyword-based methods. The generator model could be a standard model like T5 or BART. Training might involve fine-tuning the retriever and generator together, using techniques like maximum likelihood estimation or reinforcement learning.\n",
      "\n",
      "Examples of RAG in action could include systems like Google's Meena or other advanced chatbots that can access specific databases. Another example might be a RAG-based system that helps in legal research by retrieving relevant case law and generating summaries.\n",
      "\n",
      "Wait, I should also think about the limitations. Since the model relies on external data, it's dependent on the quality of that data. If the documents are outdated or incorrect, the answers will be too. Also, there's the issue of data privacy if the external data sources contain sensitive information. Handling large volumes of documents efficiently is another challenge.\n",
      "\n",
      "In terms of architecture, maybe the system first processes the query to understand what's being asked, then uses an index to find the most relevant documents. The generator then uses both the query and the retrieved documents to form a response. Techniques like query rewriting or document filtering might be used to improve retrieval accuracy.\n",
      "\n",
      "So, to summarize, RAG applications combine retrieval of external information with the generation of responses to enhance the accuracy and relevance of AI models, especially in scenarios requiring up-to-date or specialized knowledge. They have various use cases but come with their own set of challenges related to data quality, integration, and efficiency.\n",
      "</think>\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is a hybrid approach that combines **information retrieval** and **language generation** to enhance the capabilities of AI models, particularly in scenarios requiring up-to-date, specialized, or external knowledge. Below is a structured explanation of RAG applications, their components, use cases, and challenges.\n",
      "\n",
      "---\n",
      "\n",
      "### **Core Concept**\n",
      "RAG systems address the limitations of traditional language models (LLMs) that rely solely on pre-trained knowledge. Instead, RAG dynamically retrieves relevant external information (e.g., documents, databases) during inference and uses it to generate contextually accurate responses. This integration allows models to:\n",
      "- Provide **factual accuracy** by leveraging real-time or domain-specific data.\n",
      "- Avoid hallucinations by grounding answers in retrieved sources.\n",
      "- Adapt to **changing information** (e.g., current events, company policies).\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Components**\n",
      "1. **Retrieval Module**:\n",
      "   - **Function**: Retrieves relevant documents or data from a structured or unstructured corpus.\n",
      "   - **Techniques**:\n",
      "     - **Keyword-based methods** (e.g., BM25, TF-IDF).\n",
      "     - **Dense retrieval** (e.g., neural embeddings using models like DPR, BERT).\n",
      "     - **Hybrid approaches** combining both.\n",
      "   - **Indexing**: Preprocessed data is stored in vector databases (e.g., FAISS, Pinecone) or search engines (e.g., Elasticsearch) for fast query resolution.\n",
      "\n",
      "2. **Generation Module**:\n",
      "   - **Function**: Synthesizes retrieved information into coherent, concise answers.\n",
      "   - **Models**: Large language models (LLMs) like T5, BART, or GPT variants.\n",
      "   - **Training**: Jointly fine-tuned with the retriever to optimize for task-specific metrics (e.g., MRR, ROUGE).\n",
      "\n",
      "3. **Integration**:\n",
      "   - **Query processing**: The user‚Äôs input is analyzed to determine retrieval needs.\n",
      "   - **Context fusion**: Retrieved documents are combined with the query to generate the final response.\n",
      "   - **Post-processing**: Ensures relevance, filters noise, or resolves ambiguities.\n",
      "\n",
      "---\n",
      "\n",
      "### **Applications of RAG**\n",
      "1. **Customer Support**:\n",
      "   - Chatbots accessing FAQs, product manuals, or case histories to resolve issues accurately.\n",
      "\n",
      "2. **Research and Development**:\n",
      "   - Academic or scientific assistants retrieving papers, datasets, or patents for literature reviews.\n",
      "\n",
      "3. **Healthcare**:\n",
      "   - Clinical decision support systems using patient records, medical guidelines, or research studies.\n",
      "\n",
      "4. **Legal Tech**:\n",
      "   - Legal assistants summarizing case law, contracts, or regulations for legal analysis.\n",
      "\n",
      "5. **Enterprise Knowledge Management**:\n",
      "   - Internal tools for employees to search company databases, policies, or project documentation.\n",
      "\n",
      "6. **Dynamic Q&A Systems**:\n",
      "   - Answering questions about current events, stock prices, or weather by pulling live data.\n",
      "\n",
      "---\n",
      "\n",
      "### **Advantages**\n",
      "- **Accuracy**: Grounds answers in verified sources.\n",
      "- **Up-to-date responses**: Incorporates real-time or domain-specific data.\n",
      "- **Scalability**: Efficiently handles large corpora using retrieval indices.\n",
      "- **Customization**: Tailors responses to specific domains (e.g., finance, law).\n",
      "\n",
      "---\n",
      "\n",
      "### **Challenges and Limitations**\n",
      "1. **Data Quality**:\n",
      "   - Performance depends on the accuracy and relevance of retrieved documents.\n",
      "   - Outdated or conflicting sources can lead to errors.\n",
      "\n",
      "2. **Integration Complexity**:\n",
      "   - Requires coordination between retrieval and generation components.\n",
      "   - Latency from retrieval steps may affect real-time applications.\n",
      "\n",
      "3. **Privacy and Security**:\n",
      "   - Sensitive data in retrieved documents may necessitate anonymization or access controls.\n",
      "\n",
      "4. **Cost**:\n",
      "   - Computationally expensive for large-scale deployments (e.g., indexing, real-time retrieval).\n",
      "\n",
      "5. **Hallucination Risk**:\n",
      "   - While reduced compared to standalone LLMs, misinterpretation of retrieved data can still occur.\n",
      "\n",
      "---\n",
      "\n",
      "### **Technical Workflow**\n",
      "1. **Query Analysis**: Parse the user‚Äôs input to identify intent and key entities.\n",
      "2. **Document Retrieval**: Use a retriever to fetch top-k relevant documents.\n",
      "3. **Contextual Generation**: Feed the query and retrieved documents into a generator to produce the response.\n",
      "4. **Validation**: (Optional) Cross-check the response against sources or apply filtering rules.\n",
      "\n",
      "---\n",
      "\n",
      "### **Examples of RAG Systems**\n",
      "- **Google‚Äôs Search Generative Experience (SGE)**: Combines search results with generative AI for dynamic content.\n",
      "- **Meta‚Äôs LlamaIndex**: A framework for building RAG pipelines with customizable retrievers and generators.\n",
      "- **Chatbots in Financial Services**: Access internal data to provide personalized investment advice.\n",
      "\n",
      "---\n",
      "\n",
      "### **Future Directions**\n",
      "- **Decentralized RAG**: Leveraging blockchain or federated learning for secure, privacy-preserving retrieval.\n",
      "- **Multi-modal RAG**: Integrating text, images, or audio for richer context.\n",
      "- **Self-RAG**: Systems that autonomously refine queries and iteratively retrieve information.\n",
      "\n",
      "---\n",
      "\n",
      "In summary, RAG is a powerful paradigm for bridging the gap between static pre-trained models and dynamic, real-world knowledge. Its success hinges on robust retrieval mechanisms, seamless integration with generation models, and efficient handling of domain-specific data.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(\"You are a data scientist\"),\n",
    "    HumanMessage(\"Explain about RAG applications?\")\n",
    "]\n",
    "response=model.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06bfed54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a data scientist', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Explain about RAG applications?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8792154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking how to create a REST API. Let me start by recalling what a REST API is. It's an architectural style that uses HTTP methods to perform operations on resources. So the user probably wants to know the steps involved in building one.\n",
      "\n",
      "First, I should outline the basic steps. They'll need to choose a programming language and a framework. Popular choices are Python with Flask or Django, Node.js with Express, or maybe Ruby on Rails. I should mention these as options.\n",
      "\n",
      "Next, planning the resources and endpoints is crucial. They need to define what resources exist, like users or products, and the endpoints for each. For example, GET /users to get all users. Also, HTTP methods: GET for retrieving, POST for creating, PUT for updating, DELETE for removing. It's important to emphasize using proper HTTP status codes, like 200 for success, 404 for not found, etc.\n",
      "\n",
      "Setting up the project structure. They'll need to install dependencies, maybe set up a database if needed. For example, with Python, using pip to install Flask and Flask-RESTful. Then creating the app and defining routes.\n",
      "\n",
      "Implementing the endpoints. Writing the functions that handle each HTTP method. For instance, a function to handle GET requests for /users. Including JSON responses is key since REST APIs typically use JSON.\n",
      "\n",
      "Testing the API. They can use Postman or curl to send requests and check the responses. Also, maybe mention automated testing with tools like pytest or Jest.\n",
      "\n",
      "Documentation. The user should document their API endpoints, parameters, and expected responses. Swagger/OpenAPI can help generate documentation automatically.\n",
      "\n",
      "Deployment. They might need to deploy the API on a server or cloud service. Options like Heroku, AWS, or using Docker.\n",
      "\n",
      "Security considerations. Authentication with tokens or OAuth, input validation, HTTPS for secure communication.\n",
      "\n",
      "Wait, the user might not have mentioned security, but it's important to include it. Also, maybe scalability and error handling. Let me make sure to mention those points.\n",
      "\n",
      "Let me think about possible follow-up questions. They might not know which framework to choose, so giving examples for different languages would help. Also, explaining the difference between REST and other architectures briefly might be useful.\n",
      "\n",
      "I should structure the answer step by step, starting from choosing a framework, planning the API, setting up the project, writing the code, testing, documenting, deploying, and securing. Each step with a simple example. Maybe a Python example using Flask since it's straightforward for beginners.\n",
      "\n",
      "Wait, the user might be looking for a general approach rather than a specific language. But providing code examples in a particular language can make it more concrete. Let me use Python with Flask as the example since it's common for tutorials.\n",
      "\n",
      "Also, mention that the example is simple and real-world APIs would have more features like authentication, error handling, database integration, etc.\n",
      "\n",
      "Let me check if I missed anything. Oh, database integration. Even though the user didn't ask for it, connecting to a database is common when creating a REST API. Maybe include a brief note on that.\n",
      "\n",
      "Alright, putting it all together in a clear, step-by-step manner with code examples and explanations.\n",
      "</think>\n",
      "\n",
      "Creating a REST API involves defining endpoints that handle HTTP requests and return data in JSON or XML format. Below is a step-by-step guide using **Python with Flask**, a popular framework for building lightweight REST APIs.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 1: Install Requirements**\n",
      "Install Flask using pip:\n",
      "```bash\n",
      "pip install Flask\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 2: Plan Your API**\n",
      "Define your **resources** (e.g., `users`, `products`) and **endpoints** (URLs). For example:\n",
      "\n",
      "| HTTP Method | Endpoint          | Description                     |\n",
      "|-------------|-------------------|---------------------------------|\n",
      "| GET         | `/users`          | Retrieve all users              |\n",
      "| GET         | `/users/<id>`     | Retrieve a specific user by ID  |\n",
      "| POST        | `/users`          | Create a new user               |\n",
      "| PUT         | `/users/<id>`     | Update a user                   |\n",
      "| DELETE      | `/users/<id>`     | Delete a user                   |\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 3: Write the Code**\n",
      "Here‚Äôs a simple Flask app to implement the `/users` endpoints:\n",
      "\n",
      "```python\n",
      "from flask import Flask, request, jsonify\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "# In-memory \"database\"\n",
      "users = [\n",
      "    {\"id\": 1, \"name\": \"Alice\"},\n",
      "    {\"id\": 2, \"name\": \"Bob\"}\n",
      "]\n",
      "\n",
      "# 1. GET all users\n",
      "@app.route('/users', methods=['GET'])\n",
      "def get_users():\n",
      "    return jsonify(users)\n",
      "\n",
      "# 2. GET a specific user by ID\n",
      "@app.route('/users/<int:user_id>', methods=['GET'])\n",
      "def get_user(user_id):\n",
      "    user = next((user for user in users if user['id'] == user_id), None)\n",
      "    if user is None:\n",
      "        return jsonify({\"error\": \"User not found\"}), 404\n",
      "    return jsonify(user)\n",
      "\n",
      "# 3. POST a new user\n",
      "@app.route('/users', methods=['POST'])\n",
      "def create_user():\n",
      "    if not request.json or 'name' not in request.json:\n",
      "        return jsonify({\"error\": \"Invalid input\"}), 400\n",
      "    new_user = {\n",
      "        \"id\": users[-1]['id'] + 1 if users else 1,\n",
      "        \"name\": request.json['name']\n",
      "    }\n",
      "    users.append(new_user)\n",
      "    return jsonify(new_user), 201\n",
      "\n",
      "# 4. PUT (update) a user\n",
      "@app.route('/users/<int:user_id>', methods=['PUT'])\n",
      "def update_user(user_id):\n",
      "    user = next((user for user in users if user['id'] == user_id), None)\n",
      "    if user is None:\n",
      "        return jsonify({\"error\": \"User not found\"}), 404\n",
      "    if not request.json or 'name' not in request.json:\n",
      "        return jsonify({\"error\": \"Invalid input\"}), 400\n",
      "    user['name'] = request.json['name']\n",
      "    return jsonify(user)\n",
      "\n",
      "# 5. DELETE a user\n",
      "@app.route('/users/<int:user_id>', methods=['DELETE'])\n",
      "def delete_user(user_id):\n",
      "    user = next((user for user in users if user['id'] == user_id), None)\n",
      "    if user is None:\n",
      "        return jsonify({\"error\": \"User not found\"}), 404\n",
      "    users.remove(user)\n",
      "    return jsonify({\"result\": \"User deleted\"})\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(debug=True)\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 4: Test the API**\n",
      "Use tools like [Postman](https://www.postman.com/) or `curl` to test endpoints:\n",
      "\n",
      "#### **GET all users**\n",
      "```bash\n",
      "curl http://localhost:5000/users\n",
      "```\n",
      "\n",
      "#### **Create a user**\n",
      "```bash\n",
      "curl -X POST -H \"Content-Type: application/json\" -d '{\"name\": \"Charlie\"}' http://localhost:5000/users\n",
      "```\n",
      "\n",
      "#### **Update a user**\n",
      "```bash\n",
      "curl -X PUT -H \"Content-Type: application/json\" -d '{\"name\": \"Alice Smith\"}' http://localhost:5000/users/1\n",
      "```\n",
      "\n",
      "#### **Delete a user**\n",
      "```bash\n",
      "curl -X DELETE http://localhost:5000/users/1\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 5: Add Documentation**\n",
      "Use tools like [Swagger](https://swagger.io/) or [Postman API Docs](https://learning.postman.com/docs/api-documentation/) to generate interactive API documentation.\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 6: Deploy the API**\n",
      "Deploy your API using services like:\n",
      "- **Heroku** (`heroku create` and push your code).\n",
      "- **AWS/GCP/Azure**: Use serverless functions or virtual machines.\n",
      "- **Docker**: Containerize your app for production.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Concepts**\n",
      "1. **HTTP Methods**:\n",
      "   - `GET`: Retrieve data.\n",
      "   - `POST`: Create data.\n",
      "   - `PUT`: Update data.\n",
      "   - `DELETE`: Delete data.\n",
      "   - `PATCH`: Partial update.\n",
      "\n",
      "2. **Status Codes**:\n",
      "   - `200`: OK (success).\n",
      "   - `201`: Created.\n",
      "   - `400`: Bad Request (invalid input).\n",
      "   - `404`: Not Found.\n",
      "   - `500`: Internal Server Error.\n",
      "\n",
      "3. **JSON**:\n",
      "   Use `jsonify()` in Flask to return JSON responses.\n",
      "\n",
      "---\n",
      "\n",
      "### **Advanced Features**\n",
      "- **Authentication**: Use tokens (JWT) or API keys.\n",
      "- **Database Integration**: Connect to databases like PostgreSQL or MongoDB.\n",
      "- **Error Handling**: Add global exception handlers.\n",
      "- **Rate Limiting**: Prevent abuse of your API.\n",
      "\n",
      "---\n",
      "\n",
      "### **Example Output**\n",
      "```json\n",
      "// GET /users\n",
      "[\n",
      "  {\"id\": 1, \"name\": \"Alice\"},\n",
      "  {\"id\": 2, \"name\": \"Bob\"}\n",
      "]\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "This guide provides a basic REST API. For real-world applications, consider using frameworks like **Django REST Framework** (Python), **Express.js** (Node.js), or **Spring Boot** (Java) for more features and scalability.\n"
     ]
    }
   ],
   "source": [
    "### Example 2\n",
    "system_msg=SystemMessage(content=\"You are a helpful assistant.\")\n",
    "\n",
    "messages=[\n",
    "    system_msg,\n",
    "    HumanMessage(\"How do I create a REST API?\")\n",
    "]\n",
    "response= model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b1bba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### message metadata\n",
    "human_msg= HumanMessage(\n",
    "    content=\"Hello!\",\n",
    "    name=\"alice\",\n",
    "    id=\"msg_123\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f012ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user just said \"Hello!\" so the first step is to respond in a friendly and welcoming manner. I need to acknowledge their greeting and offer assistance. Let me think about a natural way to reply.\\n\\nI should keep it simple and open-ended to encourage them to ask a question or share what they need help with. Maybe something like, \"Hello! How can I assist you today?\" That allows them to specify their request. Also, I want to make sure my tone is approachable and ready to help. Let me check if there\\'s anything else I should consider. No, that should be good for now.\\n</think>\\n\\nHello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 140, 'prompt_tokens': 10, 'total_tokens': 150, 'completion_time': 0.328257538, 'completion_tokens_details': None, 'prompt_time': 0.000378454, 'prompt_tokens_details': None, 'queue_time': 0.158539165, 'total_time': 0.328635992}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2560-e26f-77b0-b71b-1dd64ed06ed2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 10, 'output_tokens': 140, 'total_tokens': 150})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke([\n",
    "    human_msg\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e9cdfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked \"what's 2+2?\" after I offered help. Let me think. First, this is a basic arithmetic question. The answer is 4, but maybe they want a detailed explanation? Wait, maybe they're testing if I can handle simple math. Let me confirm. The user might be a child or someone learning math, so I should explain clearly.\n",
      "\n",
      "I should start by stating the answer directly so they don't have to read through a long explanation if they just want the number. Then, break it down. 2 plus 2 is like adding two apples and two more apples, totaling four apples. Visual examples help. Maybe use fingers or objects. Also, mention that in different contexts, like binary, 2+2 might mean something else, but in standard decimal, it's 4. But since they didn't specify a context, stick with the standard answer. Keep it friendly and offer further help in case they need more examples or have other questions. Make sure the tone is positive and encouraging. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "The answer to 2 + 2 is **4**! üòä\n",
      "\n",
      "Here's a simple breakdown:\n",
      "- 2 (two) + 2 (another two) = 4 (four total).\n",
      "- Think of it as combining two apples with two more apples: üçéüçé + üçéüçé = üçéüçéüçéüçé.\n",
      "\n",
      "Let me know if you'd like more examples or have other questions! üöÄ\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "#create an ai message manually\n",
    "ai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n",
    "\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Can you help me?\"),\n",
    "    ai_msg,#insert as if it came from the model\n",
    "    HumanMessage(\"Great! what's 2+2?\")\n",
    "]\n",
    "\n",
    "response= model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef40d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_tokens': 54, 'output_tokens': 313, 'total_tokens': 367}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef44cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "ai_message= AIMessage(\n",
    "    content=[],\n",
    "    tool_calls=[{\n",
    "        \"name\": \"get_weather\",\n",
    "        \"args\":{\"location\":\"San Francisco\"},\n",
    "        \"id\":\"call_123\"\n",
    "    }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdd22037",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_result=\"Sunny, 72F\"\n",
    "tool_message = ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"call_123\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9059a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue conv\n",
    "\n",
    "messages=[\n",
    "    HumanMessage(\"What's the weather in San Francisco?\"),\n",
    "    ai_message,\n",
    "    tool_message,\n",
    "]\n",
    "\n",
    "response=model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe3c010c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user asked for the weather in San Francisco. I called the get_weather function with the location set to San Francisco. The response came back as \"Sunny, 72F\". Now I need to present this information clearly.\\n\\nFirst, I should confirm the location to make sure there\\'s no confusion. Then, state the current weather condition and temperature. Since the temperature is in Fahrenheit, maybe mention that it\\'s a comfortable temperature. I should keep it concise and friendly. Let me check if there\\'s any additional info needed, but since the user only asked for the weather, sticking to the provided details should be fine. Alright, time to put it all together.\\n</think>\\n\\nThe weather in San Francisco is currently sunny with a temperature of 72¬∞F. It\\'s a pleasant day! üåû', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 168, 'prompt_tokens': 57, 'total_tokens': 225, 'completion_time': 0.326460101, 'completion_tokens_details': None, 'prompt_time': 0.002184827, 'prompt_tokens_details': None, 'queue_time': 0.056680001, 'total_time': 0.328644928}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c2560-e806-74f1-97f2-52a032b3d3b5-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 57, 'output_tokens': 168, 'total_tokens': 225})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c4242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
